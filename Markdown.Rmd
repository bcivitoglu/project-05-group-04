---
title: "Cancer methylome analysis CLL vs. B-Cells"
author: "Leona Brandl, Carlotta Brueggen, Tim Kuehn, Violetta Schaaf"
date: "17th July 2019"
output: html_document
---


The aim of this project is to find genes which differ in their methylation level for cancer cells derived from chronic lymphocytic leukemia (CLL) patients compared to B-cells derived from healthy patients. Therefore, we analyze methylation data of five CLL patients and five healthy patients, looking for differentially methylated regions which we then examine for statistical and biological significance in carcinogenesis and especially inspect genes which according to literature are connected to CLL pathogenesis.
Chronic lymphocytic leukemia is the most common form of leukemia in adults and can be characterized as the uncontrolled proliferation of B-lymphocytes. Since their ability to undergo apoptosis is inhibited, these functionally incompetent B-cells accumulate in the blood vessels and displace healthy blood cells. Consequences of this development are increased vulnerability for infections, a lower oxygen level in the blood and longer bleeding periods in case of injury. As CLL originates from B-cells, we use these for comparison in our analysis.
It is proved that aberrant methylation has an effect on oncogenesis and there is evidence that this might also be the case for CLL. The most common form of methylation is the conversion of a cytosine to 5-methylcytosine by the addition of a methyl group in fifth position of the nucleobase by the DNA methyltransferase. Methylation is highly relevant in epigenetics since it has an effect on the transcription of genes and may lead to gene silencing.
In CLL, a global hypomethylation compared to healthy cells has been observed, with possible consequences being activation of gene expression as well as genomic instability.
Moreover, hypermethylation of gene promotors was frequently detected, which might lead to inhibition of promotor activity and affect the transcription of tumor suppressor genes.
The methylation data we work with was obtained by bisulfite sequencing, which is a library strategy providing information on the degree of DNA methylation. In this technique DNA is treated with bisulfite upon amplification with PCR, resulting in the conversion of unmethylated cytosines into thymines while methylated cytosines are protected from modification. Afterwards, it is possible to calculate the beta value, which is defined as the ratio of methylated cytosines to the total number of cytosines in the genome, ranging between 0 and 1. 


The analysis will be done in several steps:


1. **Data processing**
  * Reorganisation of the data
  * Quality control 
  concerning NAs, coverage values and unusefull DNA-regions
2. **Normalisation**
  * Preparing beta-values
  * Transformation of beta-values into M-values
3. **Data reduction**
  * Dimensionality reduction via PCA
  * Batch effect analysis
4. **Clustering**
  * Via K-means clustering
5. **Identifying DMRs**
  * t-test 
  * p-value correction
  * Statistical and biological relevance
  * Hypo- and hypermethylation
  
6. **Logistic regression**

7. **Data interpretation**


### Data processing


Before we can start analysing our data set it has to be tidied up to make it easy for us to work with it by making it as clear as possible. Datasets have to be split up and reorganized, coloums removed or renamed and a quality control concering coverage values, beta values, NAs and specific DNA segments showing unusual values, needs to be performed.


#### Reading in the datasets

We start by **reading in our CLL-Bcell-data and the annotations** which is an extra document explaining where our data is coming from.

```{r}
input_data <- readRDS(file ="CLL-Bcells_list.RDS.gz")
annotation <- read.csv("sample_annotation.csv")
```


#### Reorganisation of the data

Our main dataset, the one containing the data we are working with, is `input_data`. It contains methylation data in form of beta values and coverage values of B-cells of five healthy patients and five patients suffering from CLL. It is divided into four subgroups: tiling, genes, promoters and CpG islands. We have to analyse those subgroups separatly, because methylation in promoters, genes and CpG islands can have different biological reasons and also this way the data the computer deals with is not that much while running the algorithm. Therefor **the dataset gets divided into four subdatasets**.

```{r}
genes <- input_data$genes
promoters <- input_data$promoters
cpgislands <- input_data$cpgislands
tiling <- input_data$tiling
```

Those subgroups datasets contain methylation data for different regions on the genome:
`genes`, `promoters` and `cpgislands`contain genome segments with specific functions and methylation features. Genes, especially highly expressed genes, sometimes show methylation in their gene body. Unfortunately the biological function behind this is not fully clear yet. Still it can be interesting investigating in their methylation pattern in terms of cancer. In promoters  we already know that methylation plays an important role, because as promoters are the starting point for transcription, their methylation can silence genes laying downstream of these promoters. This way for example tumor suppresor genes don't get expressed any more, which can lead to cancer. Here we expect to find crucial differences in methylation between sick and healthy cells. CpG islands also are important regions on the DNA with a high density of cytosines in neighbourhood to guanines. These CpG islands are often in promoter regions of housekeeping genes and in healthy beeings only rarely methylated, because their methylation usually leads to mistakes in replication and transcription and quickly in death just because functioning promoters are so essential in those genes. Also in those CpG islands a loss of function can lead to cancer, so we expect significant methylation differences. The fourth subgroup is `tiling` which describes a specific section on the genome in steps of 5000 nucleotides without a biological context.

As we want to analyse those subgroups separatly, we first just choose one subgroup, `genes`, 
to work on our code. Later we can transfer the code on the other subgroups, too.
In this `genes`dataset we **rename the columns**, which stand for different patients (patient 1 to patient 10). They get short precise names to let the reader get the meaning on the first sight.

```{r}
colnames(genes)[11:30] <- c("P1_healthy_beta","P2_healthy_beta","P3_healthy_beta","P4_healthy_beta","P5_healthy_beta","P6_CLL_beta","P7_CLL_beta","P8_CLL_beta","P9_CLL_beta","P10_CLL_beta","P1_healthy_coverage","P2_healthy_coverage","P3_healthy_coverage","P4_healthy_coverage","P5_healthy_coverage","P6_CLL_coverage","P7_CLL_coverage","P8_CLL_coverage","P9_CLL_coverage","P10_CLL_coverage")
```


####Quality control

It is important to remove values that are not needed for or even hinder further analysis. Among other values those are also coverage values, which seem to be unrealistically high due to PCR dublicates or repetetive regions. Also they can be or that low, so the belonging beta-value of the DNA section has no meaning, because there was no fitting allignment when mapping back PCR fragments onto the genome. 
A **data frame only containing coverage values** is created.

```{r}
cov_genes <- genes[,c(1,21:30)]
```

At this point we already delete some genes, which are the genes on the X- and Y-chromosome. Somehow there must have been problems in obtaining the beta and coverage values for those chromosomes, because the beta-values are either not a number (NaN) or mainly 1.000, which must definitely be a mistake.The belonging coverage is in most cases 0, so we can not use those beta-values values anyway. Following code **removes the X- and Y-chromosome**. 

#####Removing genes on X- and Y-chsomosome

```{r}
cov_genes_new <- cov_genes[-which(cov_genes =="chrX"),]
cov_genes <- cov_genes_new[-which(cov_genes_new =="chrY"),]
rm(cov_genes_new) 
cov_genes <- cov_genes[ ,c(2:11)]
```


#####Finding a coverage value threshold

But back to the very high or low coverage values: There need to be a threshold defining a senseful coverage value range. To determine a threshold we have a look at the coverage value distribution among all patients using a logarithmic density plot of the coverage means of all patients for each genome segment. Therefor a **matrix containing the means** in generated.
```{r}
cov_genes_means <- rowMeans(data.matrix(cov_genes))
```

A **logarithmic density plot** using this matrix values is generated.
To **find an upper threshold** for deleting coverage values we try quantile values we read about in literature and draw the lines for those quantiles in the logarithmic plot to see wether one of those coincidences with a kink in the curve. For the **lower threshold** we draw lines at the values 10 and 15, which are common threshold values as we read in litearture.

```{r}
plot(density(log10(cov_genes_means)), xlab = "Coverage means", main = "Coverage distribution")
quantile(cov_genes_means, probs = c(.95))
abline(v=log10(52529.75))
quantile(cov_genes_means, probs = c(.975))
abline(v=log10(82556.55))
quantile(cov_genes_means, probs = c(.999))
abline(v=log10(311230.4))
abline(v=log10(10))
abline(v=log10(15))
```

Both lower thresholds and the highest upper threshold seem to be fitting looking at the diagram. But which threshold we will chose eventually also depends on the percentage of deleted rows at the end of quality control. It shouldn't be deleted more than 15 % of the rows, because PCA will reduce the amount even more and we don't want to lose too much information.

The procedure will now be to set all the coverage values in `cov_genes`to NA, that are below or above the chosen threshold. Then all the beta-values belonging to those coverage values set to NA will be set to an NA, too. Eventually we will set another threshold for the beta-values, defining above which number of NA per row (meaning gene), the row will be deleted. Therefor we will need two nested for-loops.


#####Preparing the first nested for-loop

Before we can set a threshold, the few **NAs among the coverage values need to be set to 0**. This way they will be set to NAs again after the loop, but they also don't hinder the loop from working.

```{r}
sum(is.na(cov_genes))
cov_genes[is.na(cov_genes)] <- 0
```

A **threshold can be chosen**. As the lower threshold we choose the coverage value 15, which is positioned at a kink of the coverage distribution diagram. As the upper coverage threshold we choose the 90 % quantile. Although it is not positioned at a kink in the diagram we consider it to be a good threshold, because it still only leads to a removal of 7(?) % in the end of the quality control, which is far underneath the upper limit of 15 %.

```{r}
threshold1 <- 15
threshold2 <- quantile(cov_genes_means, probs = seq(0.90,0.90,0.05))
```


#####The first nested for-loop

Now a **nested for-loop** can set the coverage values underneath and above the set threshold to NA.
This will be done for each row and each column of the dataframe `cov_genes`.

```{r}
for(i in 1:nrow(cov_genes)){
  for(j in 1:ncol(cov_genes)){
    if(isTRUE(cov_genes[i,j] <= threshold1)){
      cov_genes[i,j] <- NA
    } 
    if(isTRUE(cov_genes[i,j] >= threshold2)){
      cov_genes[i,j] <- NA
    }
  }
}
rm(i,j,threshold1,threshold2)
```


#####Preparing the second nested for-loop

A second nested for-loop is used to set the beta-values to NA that belong to the coverage values set to NA. Also here a **dataframe containing only beta-values** is created and the **genes located on the X- or Y-chromosome get removed**.

```{r}
beta_genes <- genes[,c(1,11:20)]
beta_genes_new <- beta_genes[-which(beta_genes =="chrX"),]
beta_genes <- beta_genes_new[-which(beta_genes_new =="chrY"),]
rm(beta_genes_new)
beta_genes <- beta_genes [ ,c(2:11)]
```

#####The second nested for-loop

This **second nested for-loop** checks every row and every column of the dataframe `cov_genes`.
If there is an NA, it sets the belonging beta-value in `beta_genes` to NA, too.

```{r}
for(k in 1:ncol(beta_genes)){
  for(l in 1:nrow(beta_genes)){
    if(isTRUE(is.na(cov_genes[k,l] == TRUE))){
      beta_genes[k,l] <- NA
    } 
  }
}
rm(k,l)
```


#####Finding a threshold for NAs among the beta-values

Now the second threshold in quality control needs to be chosen. This one is for the upper and lower threshold of allowed NAs per row (gene) among the beta-values.
Since some beta-values were set to NA in the last step, we want to get insight in **how many NAs we actually have now**.

```{r}
rmv.rows_beta_genes = apply(beta_genes,1, function(x){sum(is.na(x))})
```

A new dataframe containing only beta-values for genes which are inside the threshold is created.
We chose a threshold of 3 allowed NAs per row (gene), so **every row containing more than 3 NAs gets removed**.

```{r}
beta_genes_cleaned <- beta_genes[-which(rmv.rows_beta_genes >2),]
```

To see **how many rows are gone** due to quality control the numer of rows before and after get compared. Also we hava a look at the **percantage of rows deleted** and see wether it is beneath 15 %.

```{r}
row_difference = nrow(genes)-nrow(beta_genes_cleaned)
sum(row_difference)
genes_deleted_percentage = row_difference/nrow(genes)*100
sum(genes_deleted_percentage)
```

With 7% we are beneath the recomended upper limit of 15 % of deleted rows.


###Normalisation

The tidy beta-values we have now are an approximation of the percentage of DNA-methylation. Biologically beta-values are easy to understand. They range from 0 to 1, while 0 is unmethylated and 1 is fully methylated. But their bounded range is also a disadvantage. It leads to the problem that they can not be used for statistical tests like the t-test, because they violate the Gaussian distribution. For highly methylated gene regions and unmethylated gene regions beta-values are very heteroscedastic, which means, that the variability of a variable is unequal across the range of values. Therefor we do a normalisation. Beta-values need to be transformed into M-values via a logit transformation. M-values do not have a bounded range, they can be infinite big and small. While the middle methylation range (beta-value range of 0,2-0,8) is linear to M-values, outside this range the beta-values are compressed and the M-values more accurate and homoscedastic. This is why M-values can be used for statistical tests and are absolutely necessary for further analysis.

####Preparing beta-values for logit transformation

Logit transformation turns the extreme beta-values 0 and 1 into -infinite and infinite. We can not work with those values, because further tests don't know how to deal with an infinite number. Due to that we apply a little trick and **set 0 to a very small number higher than 0, and 1 to a very big number less than 1**.

```{r}
beta_genes_cleaned[beta_genes_cleaned==0]<-0.00000001
beta_genes_cleaned[beta_genes_cleaned==1]<-0.99999999
```


#####Creating two seperate data frames for sick and healthy patients

For doing our Normalisation were we transform our beta-values into M-values we have to **split up our data into beta-values of healthy and sick patients.**

```{r}
beta_genes_healthy <- beta_genes_cleaned[,c(1:5)]
beta_genes_cancer <- beta_genes_cleaned[,c(6:10)]
```

Now we are able to **replace the remaining NA's by the row means of the different genes.** We suspect that the rowmeans differ between healthy and sick patients so it is important to work with the two dataframes.


```{r}
k <- which(is.na(beta_genes_healthy), arr.ind=TRUE)
beta_genes_healthy[k] <- rowMeans(beta_genes_healthy, na.rm=TRUE)[k[,1]]
l <- which(is.na(beta_genes_cancer), arr.ind=TRUE)
beta_genes_cancer[l] <- rowMeans(beta_genes_cancer, na.rm=TRUE)[l[,1]]
```


####Transforming beta-values into M-values

In the next step we **transform our beta-values into M-values** so we are able to do statistical tests with the data. We calculate the M-values with the equation M-value = log2(beta-value/(1??? beta-value)).

```{r}
M_genes_healthy <- log2(beta_genes_healthy/(1-(beta_genes_healthy)))
M_genes_cancer <- log2(beta_genes_cancer/(1-(beta_genes_cancer)))
```


For applying the PCA on our data we have to build a **dataset which contains sick and healthy patients** with **short column names**.

```{r}
M_genes <- cbind(M_genes_healthy, M_genes_cancer)
colnames(M_genes) <- c("1H","2H","3H","4H","5H","6CLL","7CLL","8CLL","9CLL","10CLL")
```


###Dimensionality reduction

To **reduce our data** we use Principal Component Analysis. PCA reduces a large set of variables into a smaller set which still contains most information of the large dataset. Through the PCA, correlated variables get transformed into a much smaller number of uncorrelated variables which are called principal components.
We want to have less variables, because it is easier to work with them. 


####PCA

The code for the **PCA** expects the patients to be rows and the samples to be columns. In our data the patients are columns and the genes are rows so we have to **transpose the matrix** with the t() function. 

```{r}
pca_M <- prcomp(t(M_genes))
```


#####How much variation does each component account for?

To decide, how much principal components we want to work with, we have to know how much variation each principal component accounts for in percentage. We plot the percentage for each component and decide with the "elbow method" with which number of components we have to work with. 

```{r}
var_pca <- pca_M$sdev^2
var_pca_per <- round(var_pca/sum(var_pca)*100, 1)
plot(var_pca_per, main="Variation of our data", xlab="Principal Components", ylab="Percent Variation", ylim=c(0,25),type = "o", pch=20)
```

In the plot we can not see an elbow so we would actually have to work on with ten principal components. But as the last PCs don't explain much variation anyway and are therefor useless for us, we investigate on the first five PCs.



#####Drawing a 2D plot of component 1 and 2

To check whether we have a batch effect, we plot principal component 1 against principle component 2. 

```{r}
library(ggplot2)

pca_values <- data.frame(Sample=rownames(pca_M$x),
                       X=pca_M$x[,1],
                       Y=pca_M$x[,2])
ggplot(data=pca_values, aes(x=X, y=Y, label=Sample)) +
  geom_text(aes(colour = annotation$DISEASE)) +
  xlab(paste("PC1 - ", var_pca_per[1], "%", sep="")) +
  ylab(paste("PC2 - ", var_pca_per[2], "%", sep="")) +
  theme_bw() +
  ggtitle("PCA Graph of principal component 1 and 2")
```



####Investigation for batch effect 

Now we use different categories of the annotation table which possibly might cause **batch effect** and divide them by colour and shape in our graph of principal component 1 and 2.
Batch effect is a **technical source of error**, which could have been added to the samples by e.g. gender. It is important that the technical variation does not confound with the biology to not falsify the interpreted data.
To avoid errors, we used three tests to see whether there is a batch effect in a principal component or not.

```{r}
ggplot_1 <- ggplot(data=pca_values, aes(x=X, y=Y, label=Sample)) +
  geom_point(aes(shape=annotation$cellTypeGroup, color=annotation$Predicted.Gender)) +
  xlab(paste("PC1 - ", var_pca_per[1], "%", sep="")) +
  ylab(paste("PC2 - ", var_pca_per[2], "%", sep="")) +
  theme_bw(base_size = 6) +
  ggtitle("PC1&2 check for gender")

ggplot_2 <- ggplot(data=pca_values, aes(x=X, y=Y, label=Sample)) +
  geom_point(aes(shape=annotation$cellTypeGroup, color=annotation$SAMPLE_DESC_3)) +
  xlab(paste("PC1 - ", var_pca_per[1], "%", sep="")) +
  ylab(paste("PC2 - ", var_pca_per[2], "%", sep="")) +
  theme_bw(base_size = 6) +
  ggtitle("PC1&2 check for cell type/origin")

ggplot_4 <- ggplot(data=pca_values, aes(x=X, y=Y, label=Sample)) +
  geom_point(aes(shape=annotation$cellTypeGroup, color=annotation$DONOR_AGE)) +
  xlab(paste("PC1 - ", var_pca_per[1], "%", sep="")) +
  ylab(paste("PC2 - ", var_pca_per[2], "%", sep="")) +
  theme_bw(base_size = 6) +
  ggtitle("PC1&2 check for age")

ggplot_3 <- ggplot(data=pca_values, aes(x=X, y=Y, label=Sample)) +
  geom_point(aes(shape=annotation$cellTypeGroup, color=annotation$BIOMATERIAL_PROVIDER)) +
  xlab(paste("PC1 - ", var_pca_per[1], "%", sep="")) +
  ylab(paste("PC2 - ", var_pca_per[2], "%", sep="")) +
  theme_bw(base_size = 6) +
  ggtitle("PC1&2 check for biomaterial provider")

library(gridExtra)
library(ggplot2)
grid.arrange(ggplot_1,ggplot_2, ncol=2)
grid.arrange(ggplot_3,ggplot_4, ncol=2)
```


#####Checking our PC 1 to 5 for significant batch effect per category 

To verify our observations, we want to test if batch effect of the different categories is significant for PC 1-5. Therefore, we first create three dataframes containing PC 1-5 and the category of the annotation table we want to investigate, depending on the properties of the data, because we need to perform different tests for different data types.
For numbers we use the **permutation test** (of Pearson's r correlation coefficient), for two categories the **Wilcoxon test** and for more than two different categories the **Kruskal-Wallis test**.

```{r}
x <- pca_M[["x"]]
pca1_5 <- x[,1:5]
batch_kruskal <- data.frame(pca1_5, annotation$SAMPLE_DESC_3)
batch_wilcoxon <- data.frame(pca1_5, annotation$BIOMATERIAL_PROVIDER, annotation$DISEASE, annotation$Predicted.Gender)
batch_permutation <- data.frame (pca1_5, annotation$SEQ_RUNS_COUNT)
```


#####Permutation test for numbers

The **permutation test** is a non-parametric test to check, whether two not connected spot checks derive from the same basic total unit. The H0 hypothesis assumes that both spot checks derive from the same basic total unit. The permutation test in the usual form is tailored to the counterhypothesis that the distributions of the two samples emerge by a shift apart. Often the question is not only if there is a shift, but also how big is this shift. Basically it is possible to check for a batch effect with numbers. 

```{r}
cor.perm <- function (x, y, nperm = 1000)
{
  r.obs <- cor (x = x, y = y)
  P.par <- cor.test (x = x, y = y)$p.value
  #  r.per <- replicate (nperm, expr = cor (x = x, y = sample (y)))
  r.per <- sapply (1:nperm, FUN = function (i) cor (x = x, y = sample (y)))
  r.per <- c(r.per, r.obs)
 
  P.per <- sum (abs (r.per) >= abs (r.obs))/(nperm + 1) 
  return(list(r.obs = r.obs, P.par = P.par, P.per = P.per))
}

x <-batch_permutation$PC1
y <-batch_permutation$annotation.SEQ_RUNS_COUNT

p_PC1_SEQ <- cor.perm(x,y)

x <-batch_permutation$PC2
p_PC2_SEQ <- cor.perm(x,y)

x <-batch_permutation$PC3
p_PC3_SEQ <- cor.perm(x,y)

x <-batch_permutation$PC4
p_PC4_SEQ <- cor.perm(x,y)

x <-batch_permutation$PC5
p_PC5_SEQ <- cor.perm(x,y)

p_SEQ_RUNS_COUNT <- data.frame(p_PC1_SEQ$P.per, p_PC2_SEQ$P.per, p_PC3_SEQ$P.per, p_PC4_SEQ$P.per, p_PC5_SEQ$P.per)
```


#####Wilcoxon test for 2 categories

The **Wilcoxon test** for dependent spot checks tests, whether the central tendencies are different. "Dependent spot checks" is used if one measurement in a sample and one measurement in another sample influence each other. In three situations this is the case:
1) Repeated measures
2) Natural pairs
3) Matching
Basically it is possible to check for a batch effect with two categories only.

```{r}
pc_1_BIOMATERIAL_PROVIDER <- wilcox.test(batch_wilcoxon$PC1 ~ batch_wilcoxon$annotation.BIOMATERIAL_PROVIDER, data = batch_wilcoxon, exact = FALSE)
pc_2_BIOMATERIAL_PROVIDER <- wilcox.test(batch_wilcoxon$PC2 ~ batch_wilcoxon$annotation.BIOMATERIAL_PROVIDER, data = batch_wilcoxon, exact = FALSE)
pc_3_BIOMATERIAL_PROVIDER <- wilcox.test(batch_wilcoxon$PC3 ~ batch_wilcoxon$annotation.BIOMATERIAL_PROVIDER, data = batch_wilcoxon, exact = FALSE)
pc_4_BIOMATERIAL_PROVIDER <- wilcox.test(batch_wilcoxon$PC4 ~ batch_wilcoxon$annotation.BIOMATERIAL_PROVIDER, data = batch_wilcoxon, exact = FALSE)
pc_5_BIOMATERIAL_PROVIDER <- wilcox.test(batch_wilcoxon$PC5 ~ batch_wilcoxon$annotation.BIOMATERIAL_PROVIDER, data = batch_wilcoxon, exact = FALSE)

pc_1_BIOMATERIAL_PROVIDER <- pc_1_BIOMATERIAL_PROVIDER$p.value
pc_2_BIOMATERIAL_PROVIDER <- pc_2_BIOMATERIAL_PROVIDER$p.value
pc_3_BIOMATERIAL_PROVIDER <- pc_3_BIOMATERIAL_PROVIDER$p.value
pc_4_BIOMATERIAL_PROVIDER <- pc_4_BIOMATERIAL_PROVIDER$p.value
pc_5_BIOMATERIAL_PROVIDER <- pc_5_BIOMATERIAL_PROVIDER$p.value

pc_1_DISEASE <- wilcox.test(batch_wilcoxon$PC1 ~ batch_wilcoxon$annotation.DISEASE, data = batch_wilcoxon, exact = FALSE)
pc_2_DISEASE <- wilcox.test(batch_wilcoxon$PC2 ~ batch_wilcoxon$annotation.DISEASE, data = batch_wilcoxon, exact = FALSE)
pc_3_DISEASE <- wilcox.test(batch_wilcoxon$PC3 ~ batch_wilcoxon$annotation.DISEASE, data = batch_wilcoxon, exact = FALSE)
pc_4_DISEASE <- wilcox.test(batch_wilcoxon$PC4 ~ batch_wilcoxon$annotation.DISEASE, data = batch_wilcoxon, exact = FALSE)
pc_5_DISEASE <- wilcox.test(batch_wilcoxon$PC5 ~ batch_wilcoxon$annotation.DISEASE, data = batch_wilcoxon, exact = FALSE)

pc_1_DISEASE <- pc_1_DISEASE$p.value
pc_2_DISEASE <- pc_2_DISEASE$p.value
pc_3_DISEASE <- pc_3_DISEASE$p.value
pc_4_DISEASE <- pc_4_DISEASE$p.value
pc_5_DISEASE <- pc_5_DISEASE$p.value

pc_1_Predicted.Gender <- wilcox.test(batch_wilcoxon$PC1 ~ batch_wilcoxon$annotation.Predicted.Gender, data = batch_wilcoxon, exact = FALSE)
pc_2_Predicted.Gender <- wilcox.test(batch_wilcoxon$PC2 ~ batch_wilcoxon$annotation.Predicted.Gender, data = batch_wilcoxon, exact = FALSE)
pc_3_Predicted.Gender <- wilcox.test(batch_wilcoxon$PC3 ~ batch_wilcoxon$annotation.Predicted.Gender, data = batch_wilcoxon, exact = FALSE)
pc_4_Predicted.Gender <- wilcox.test(batch_wilcoxon$PC4 ~ batch_wilcoxon$annotation.Predicted.Gender, data = batch_wilcoxon, exact = FALSE)
pc_5_Predicted.Gender <- wilcox.test(batch_wilcoxon$PC5 ~ batch_wilcoxon$annotation.Predicted.Gender, data = batch_wilcoxon, exact = FALSE)

pc_1_Predicted.Gender <- pc_1_Predicted.Gender$p.value
pc_2_Predicted.Gender <- pc_2_Predicted.Gender$p.value
pc_3_Predicted.Gender <- pc_3_Predicted.Gender$p.value
pc_4_Predicted.Gender <- pc_4_Predicted.Gender$p.value
pc_5_Predicted.Gender <- pc_5_Predicted.Gender$p.value

p_DISEASE <- data.frame(pc_1_BIOMATERIAL_PROVIDER, pc_2_BIOMATERIAL_PROVIDER, pc_3_BIOMATERIAL_PROVIDER, pc_4_BIOMATERIAL_PROVIDER, pc_5_BIOMATERIAL_PROVIDER)
p_BIOMATERIAL_PROVIDER <- data.frame(pc_1_BIOMATERIAL_PROVIDER, pc_2_BIOMATERIAL_PROVIDER, pc_3_BIOMATERIAL_PROVIDER, pc_4_BIOMATERIAL_PROVIDER, pc_5_BIOMATERIAL_PROVIDER)
p_Predicted.Gender <- data.frame(pc_1_Predicted.Gender, pc_2_Predicted.Gender, pc_3_Predicted.Gender, pc_4_Predicted.Gender, pc_5_Predicted.Gender)
```

#####Kruskal-Wallis for several categories

The **Kurskal-Wallis test** checks, whether there is a difference in central tendencies between several independent spot checks. The test is used, if the prerequisites for a parametric method is not fulfilled. One of the benefits of the test is that the data does not have to be normally distributed. On the other hand, the test can also be calculated for small samples and outliers.
Basically it is possible to check for a batch effect with several categories.

```{r}
batch_kruskal <- within(batch_kruskal, {
  PC1 <- as.numeric(as.character(PC1))
})

batch_kruskal <- within(batch_kruskal, {
  PC2 <- as.numeric(as.character(PC2))
})

batch_kruskal <- within(batch_kruskal, {
  PC3 <- as.numeric(as.character(PC3))
})

batch_kruskal <- within(batch_kruskal, {
  PC4 <- as.numeric(as.character(PC4))
})

batch_kruskal <- within(batch_kruskal, {
  PC5 <- as.numeric(as.character(PC5))
})

sample_desc_3_pc1 <- kruskal.test(batch_kruskal$PC1 ~ batch_kruskal$annotation.SAMPLE_DESC_3, data = batch_kruskal)
sample_desc_3_pc2 <- kruskal.test(batch_kruskal$PC2 ~ batch_kruskal$annotation.SAMPLE_DESC_3, data = batch_kruskal)
sample_desc_3_pc3 <- kruskal.test(batch_kruskal$PC3 ~ batch_kruskal$annotation.SAMPLE_DESC_3, data = batch_kruskal)
sample_desc_3_pc4 <- kruskal.test(batch_kruskal$PC4 ~ batch_kruskal$annotation.SAMPLE_DESC_3, data = batch_kruskal)
sample_desc_3_pc5 <- kruskal.test(batch_kruskal$PC5 ~ batch_kruskal$annotation.SAMPLE_DESC_3, data = batch_kruskal)

p_SAMPLE_DESC_3 <- data.frame(sample_desc_3_pc1$p.value, sample_desc_3_pc2$p.value, sample_desc_3_pc3$p.value, sample_desc_3_pc4$p.value, sample_desc_3_pc5$p.value)
```


We create a data frame containing **all p values of the categories** we investigate for a batch effect.

```{r}
p_DISEASE_t <- as.data.frame(t(p_DISEASE))
p_BIOMATERIAL_PROVIDER_t <- as.data.frame(t(p_BIOMATERIAL_PROVIDER))
p_Predicted.Gender_t <- as.data.frame(t(p_Predicted.Gender))
p_SEQ_RUNS_COUNT_t <- as.data.frame(t(p_SEQ_RUNS_COUNT))
p_SAMPLE_DESC_3_t <- as.data.frame(t( p_SAMPLE_DESC_3))

total_pvalue <- cbind(p_DISEASE_t,p_BIOMATERIAL_PROVIDER_t,p_Predicted.Gender_t,p_SEQ_RUNS_COUNT_t,p_SAMPLE_DESC_3_t)
```


We give columns and rows informative names.

```{r}
names(total_pvalue)[1] <- "p_DISEASE"
names(total_pvalue)[2] <- "p_BIOMATERIAL"
names(total_pvalue)[3] <- "p_GENDER"
names(total_pvalue)[4] <- "p_SEQ_RUNS_COUNT"
names(total_pvalue)[5] <- "p_SAMPLE_DESC"

rownames(total_pvalue)[rownames(total_pvalue) == "pc_1_BIOMATERIAL_PROVIDER"] <- "PC1"
rownames(total_pvalue)[rownames(total_pvalue) == "pc_2_BIOMATERIAL_PROVIDER"] <- "PC2"
rownames(total_pvalue)[rownames(total_pvalue) == "pc_3_BIOMATERIAL_PROVIDER"] <- "PC3"
rownames(total_pvalue)[rownames(total_pvalue) == "pc_4_BIOMATERIAL_PROVIDER"] <- "PC4"
rownames(total_pvalue)[rownames(total_pvalue) == "pc_5_BIOMATERIAL_PROVIDER"] <- "PC5"
```



#####Preparation for heatmap 

To **decide which PC we will use for further analysis**, we draw a heatmap showing which principle component is affected significantly by a batch effect. All significant p-values are shown in red and non-significant ones in blue.
For analyzing and comparing all p-values we state all p-values under 0.1 as significant. To make visualization easier we set values above the significance threshold to 10 and values equal or below to 1, so we just have to find colours for 2 values.

```{r}

total_pvalue <- data.matrix(total_pvalue)

for (i in 1:ncol(total_pvalue)){
  for (j in 1:nrow(total_pvalue)){
    if(isTRUE(total_pvalue[i,j] > 0.1)){
      total_pvalue[i,j] <- 10
    }
    if (isTRUE(total_pvalue[i,j] <= 0.1)){
      total_pvalue [i,j] <- 1
    }
  }
}
```


#####Heatmap 

```{r}
library(gplots)

par(cex.main = 1)
	
	heatmap.2(
	  total_pvalue, main = "Batch effect in our principal components",
	  title(main, cex.main = 1 * op[["cex.main"]]),
	  margins = c(8,5),
	  Colv = NA, 
	  Rowv = NA,
	  dendrogram = "none", 
	  sepwidth = c(0.01, 0.01), 
	  sepcolor = "black", 
	  trace= "none", 
	  col = colorRampPalette(c("salmon","blue")), 
	  breaks = c(seq(0, 1, length = 2),
	             seq(1.1, 10, length = 2)),
	  colsep = 1:ncol(total_pvalue), 
	  rowsep = 1:nrow(total_pvalue), 
	  cexCol = 0.7,
	  cexRow = 1,
	  key = FALSE)
```


By looking at the heatmap or at the p-values one can see that principal component 1 has a batch effect.
Therefore **we go on with our analysis using principal component 2** which doesn't show a batch effect at all.

#####create a dataframe with loading scores of the 10000 most importat genes, which have the most influence for principal component 2.

Now we want to **find the most important genes** which means that we want to find the genes which have the most influence on the principal component two. For that we use the loading scores which we can find in rotation of the PCA. The loadings describe the weight of each gene in the principal component. If a gene has a high loading it has a high weight in the principal component. A loading can be positive or negative but we will first have a look on the absolut values. We create a dataframe containing the 2000 and 10000 most important genes in principal component 2 in ranked order.

```{r}
loading_scores <- pca_M$rotation[,2]
gene_scores <- abs(loading_scores) 
ranked_gene_score <- sort(gene_scores, decreasing=TRUE)
genes_top_10000 <- names(ranked_gene_score[1:10000])
genes_top_2000 <- names(ranked_gene_score[1:2000])
```

#####Look for elbow in top gene variance

Now we want to **decide how many genes we want to keep for our further analysis** by looking for an elbow in the plot of loading scores. 

```{r}
plot(abs(pca_M$rotation[genes_top_10000,2]),type = "o", pch=20)
```

Actually there is a kink in the elbow plot of loading scores of the genes at round about 2000 genes. K-means clustering already worked with 2000 genes but by using only 2000 genes we get only 3 significant different methylated genes, which is not sufficient. By using 10000 genes we get 24 differentially methylated genes, which is an amount which we can work with. 


###Clustering with k-means

K-means is a method to find groups in the data. It uses an iterative algorithm which means that there is a repitition of steps with the aim of finding the right cluster for each datapoint. First step is the clustering which means that the datapoints get split up into k clusters. Each datapoint gets alloted to its next clusterpoint. Next step is an updating of the clusters by calculating center of each cluster and define them as new clusterpoints. These two steps will be repeated as long as set in the code. By performing k-means clustering we want to **check whether sick and healthy patients are separated in two different clusters.** Therefor we first create two dataframes containing all M-values from the 2000 and 10000 most important genes for principal component 2.

```{r}
k_means_data2 <- M_genes[genes_top_2000,]
k_means_data <- M_genes[genes_top_10000,]

k <- kmeans(x = t(k_means_data2), centers = 2, iter.max = 1000)
k <-
  kmeans(
    x = t(k_means_data),
    centers = 2,
    iter.max = 1000
  )

cluster <- data.frame(k[["cluster"]])
cluster
```

One can see that our sick and healthy patients belong to two different clusters so we can use principal component two for further testing.



### Identifying differetially methylated regions (DMRs)

####Student's t-test

To see whether the methylation differences, so differences in M-values, between the two Sample groups are significant, **a student t-test is performed**. It can be used, because due to normalisation the varaiance of the values are equal now. The test compares the mean M-value of sick patients to the mean M-value of healthy patients and gives a p-value, which is significant or not depending on how strict is the threshold you set.

```{r}
cluster <- k[["cluster"]]
p_value = NULL
for(i in 1:nrow(k_means_data)){
  x = k_means_data[i, 1:5]
  y = k_means_data[i, 6:10]
  t_test = t.test(x, y)
  p_value[i] = t_test$p.value
}
```

We **create a dataframe wich contains the p-values** from t-test.

```{r}
pvalues <- data.frame(p_value)
```

To get the gene ID belonging to those p-values we **change the rownames**.

```{r}
row_names <- row.names(k_means_data)
p_combined <- pvalues
rownames(p_combined) <- row_names
```


####Correction of p values by using the holm method

To reduce the family-wise error rate, which is the propability of getting one ore more type 1 errors (also called false positive error), the *holm method* is used. Besides a *dataframe* containing the uncorrected p-values and the corrected p-values in rising order and the belonging gene ID and symbol is created.

```{r}
p_combined$p_adjusted = p.adjust(p_combined$p_value, method = "holm")
p_holm <- data.frame(p_combined)
symbols <- genes[rownames(p_holm),]
holm <- cbind(p_holm, "Symbols" = symbols$symbol)
holm_new <- holm[order(holm$p_value),]
```



####Finding DMRs

Only 24 genes are significantly differently methylated using a threshold of a corrected p-value of  0,1.
Besides there have been genes, often correlated to CLL, which we found in literature before.
**Literature genes**, which we also found in our initial dataset `genes` are those ones.

```{r}
#grep("symbol", genes$symbol, value = TRUE)
```

* TWIST2
* VHL
* ABI3
* ADORA3
* PRF1
* CLLU1
* LPL
* NOTCH1
* SCARF1
* TNF
* CTBP2
* EGR1

The following genes from the literature can be found in our dataset heaped, as they probably belong to a family:

* NFAT
* EBF
* ODC


The **literature genes still existing in the dataframe after the t-test** are those ones.

```{r}
#grep("gene_name", holm_new$Symbols, value = TRUE)
```

* ENSG00000048462 -> TNFRSF17
* ENSG00000159958 -> TNFRSF13C
* ENSG00000232810 -> TNF 
* ENSG00000233125 -> ACTBP12 (Maybe not that important, because original gene out of literature was named "CTBP2")

Those ones still existing in the dataframe after the t-test and the 24 significant genes are the relevant genes which may have an impact on CLL forming. We **create a dataframe of relevant genes** and their p-valus to have an overview of genes we should do some research about.

```{r}
relevant_genes1 <- holm_new[c(1:24),]
relevant_genes2 <- holm_new[c("ENSG00000048462","ENSG00000159958","ENSG00000232810","ENSG00000233125"),]
relevant_genes <- rbind(relevant_genes1,relevant_genes2)
print(relevant_genes)
```


####Statistical and biological relevant methylation differences in genes

A vulcano plot can help visualising whether a gene is claimed to be differently methylated in healthy and sick patients because of statistical reasons, which means a significant p-value or because of biological reasons, which would be a big difference in the mean of the M-values or beta-values. Only the genes which show both characteristics, really have a significant methylation difference.

#####Fold change calculation

On the x-axis of the volcano plot there is the **difference of the beta-value means of each sample group**. This is the axis showing the biological relevance. Usually for better visualization you use the log2 fold change of the M-values. Due to the negative M-values that exist, this is mathematically not possible. Therefor we do not use the log2 fold change of the M-values, but the one of the beta-values.


```{r}
beta_10000 <- beta_genes_cleaned[genes_top_10000,]
beta_10000_log <- log2(beta_10000)
control <- beta_10000_log[, 1:5]
tumor <- beta_10000_log[, 6:10]
control_mean <- apply(control, 1, mean)
tumor_mean <- apply(tumor, 1, mean)
fold = control_mean - tumor_mean
```

#####Visualisation including information about hyper- and hypomethylation 

Also on the y-axis we have to improvise: The y-axis shows the p-values and therefor the statistical relevance. Actually the corrected p-values should be used, but somehow a lot of p-values were corrected to the value 1, which distorts the plot. Therefor we use the uncorrected p-values for the y-axis and **create a volcano plot**

```{r}
plot(fold, -log10(p_value), ylim = c(0,3))
```

To visualise genes that are statistically and biologically relevant and subdivide them into upmethylated and hypomethylated genes in case of cancer, we first **create thresholds for significance** in p-values and beta-value-difference (fold).

```{r}
threshold_significant = 0.1
threshold_fold = 1
```

We create a **filter for biological relevant genes** using the threshold for beta-value-difference, one for **statistically relevant genes** and one that **combines both in one filter**.

```{r}
filter_by_fold = abs(fold) >= threshold_fold
filter_by_pvalue = abs(p_value) <= threshold_significant
filter_combined = filter_by_fold & filter_by_pvalue
```

The genes which fit this filter and are **upmethylated** in cancer cells are coloured in red, while the **hypomethylated** genes are coloured in blue.

```{r}
plot(fold, -log10(p_value))
points (fold[filter_combined & fold < 0],
        -log10(p_value[filter_combined & fold < 0]),
        pch = 16, col = "red")

points (fold[filter_combined & fold > 0],
        -log10(p_value[filter_combined & fold > 0]),
        pch = 16, col = "blue")
```


###Logistic regression

Logistic regression is used to model the relationship between a binary outcome variable and one or more predictor variables. In this case, the predictor variables are methylation data of the genes and the outcome is either healthy or sick. Hence we want to **predict a patient's health status by the methylation status of their genes**.

We create a dataframe, which contains M values and health status of 7 random patients. Therefore, the M value dataframe needs to be transformed first. We then add the patients' health status. This data frame is used for generating the logistic regression model. We then do cross validation by applying the regression model to the test set data frame and by checking whether the predicted and actual health status match, we can see if the regression model makes works.

```{r}
log_regression <- k_means_data[,c(1,2,4,5,8,9,10)]
log_regression <- t(log_regression)
log_regression <- data.frame(log_regression)
test_set <- k_means_data[,c(3,6,7)]
test_set <- data.frame(t(test_set))

healthstatus <- annotation$DISEASE
healthstatus <- data.frame(healthstatus)
healthstatus_regression <- healthstatus[c(1, 2, 4, 5, 8, 9, 10),]

train_set <- cbind(healthstatus_regression, log_regression)

regression_model <- glm(healthstatus_regression ~ ., family = "binomial", data = train_set)
summary_regression_model <- summary(regression_model)
summary_regression_model$coefficients
```

We only get estimate values for the genes shown above whereas the values for the remainig genes are NA and therefore not shown in the table. This is because of multicollinearity, which means that one or more predictor variables are highly correlated and consequently do not contribute to the regression model.
The estimate value explains the influence of the predictor variable on the outcome variable. Positive values indicate that a gene contributes to health whereas genes with negative values promote disease.
It is notable that we get high standard errors, which implies that the regression model is not very reliable. These high standard error values result from mulitcollinearity.

```{r}
prediction <- predict(regression_model, newdata = test_set, type = "response")
prediction
levels(train_set$healthstatus_regression)
```

Since we only have very few data available, the prediction of the logistic regression model based on the train set containing 70% of our data does not correspond with the actual health status. Even though the predicted probability is 10^6 times higher for the healthy patient than for the ones with CLL, it is still very close to 0, which implies that the patient is probably sick.
Therefore we use all data available to model logistic regression without performing cross validation.

```{r}
k_means_data_no_cv <- t(k_means_data)
k_means_data_no_cv <- data.frame(k_means_data_no_cv)
full_data <- cbind(healthstatus, k_means_data_no_cv)
model_no_cv <- glm(healthstatus ~ ., family = "binomial", data = full_data)
prediction_no_cv <- predict(model_no_cv, newdata = full_data, type = "response")
summary_model_no_cv <- summary(model_no_cv)
summary_model_no_cv$coefficients
```

We now apply the second model to our data set.

```{r}
prediction_no_cv
```

The predicted probability to be healthy is 1 for the healthy patients and very close to 0 for the sick patients, so it corresponds with the actual health status. This is the result we expected, since we made the prediction for the data we used to create the model.
